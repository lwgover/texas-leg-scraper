{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SB 21    Senate Committee Report\n",
      "https://capitol.texas.gov/tlodocs/89R/analysis/html/SB00021S.htm\n",
      "https://capitol.texas.gov/tlodocs/89R/analysis/html/SB00021S.htm\n",
      "SB 262   Senate Committee Report\n",
      "https://capitol.texas.gov/tlodocs/89R/analysis/html/SB00262S.htm\n",
      "https://capitol.texas.gov/tlodocs/89R/analysis/html/SB00262S.htm\n",
      "SB 1066  Senate Committee Report\n",
      "https://capitol.texas.gov/tlodocs/89R/analysis/html/SB01066S.htm\n",
      "https://capitol.texas.gov/tlodocs/89R/analysis/html/SB01066S.htm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import yaml\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "CONFIG_PATH = './scraper_config.yaml'\n",
    "with open(CONFIG_PATH, 'r') as file:\n",
    "    scraper_config = yaml.safe_load(file)\n",
    "\n",
    "rss_url = scraper_config['sources']['rss']['daily']['bill_analyses']\n",
    "\n",
    "feed = feedparser.parse(rss_url)\n",
    "\n",
    "if feed.status == 200:\n",
    "    for entry in feed.entries:\n",
    "        print(entry.title)\n",
    "        print(entry.link)\n",
    "        print(entry.guid)\n",
    "else:\n",
    "    print(\"Failed to get RSS feed. Status code:\", feed.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# MISC UTILITY FUNCTIONS\n",
    "################################################################################\n",
    "\n",
    "def bill_text_table_to_df(soup_html):\n",
    "    \"\"\"\n",
    "    Convert an HTML table containing bill text information into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        soup_html: BeautifulSoup object containing the HTML table\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with the table data, where each row represents a bill version\n",
    "        and columns contain metadata like version type and links to bill text\n",
    "    \"\"\"\n",
    "    rows = soup_html.find_all('tr')\n",
    "    \n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # Extract column names from the first row\n",
    "    headers = [header.get_text(strip=True).replace('\\n', ' ') for header in rows[0].find_all('td')]\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for row in rows[1:]:  # Skip header row\n",
    "        cells = row.find_all('td')\n",
    "        row_dict = {}\n",
    "        \n",
    "        for i, cell in enumerate(cells):\n",
    "            images = cell.find_all('img')\n",
    "            links = cell.find_all('a')\n",
    "            \n",
    "            if images and links:\n",
    "                # Store multiple images in a list of dictionaries\n",
    "                image_data = []\n",
    "                for img, link in zip(images, links):\n",
    "                    img_dict = {\n",
    "                        \"name\": img.get(\"alt\", \"\"),\n",
    "                        \"URL\": link.get(\"href\", \"\")\n",
    "                    }\n",
    "                    image_data.append(img_dict)\n",
    "                row_dict[headers[i]] = image_data\n",
    "            else:\n",
    "                row_dict[headers[i]] = cell.get_text(strip=True)\n",
    "        \n",
    "        data_list.append(row_dict)\n",
    "    \n",
    "    return pd.DataFrame(data_list)\n",
    "\n",
    "\n",
    "def write_df_to_gsheets(df, google_sheets_id, worksheet_name):\n",
    "    \"\"\"\n",
    "    Write a pandas DataFrame to a Google Sheets worksheet.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame to write\n",
    "        google_sheets_id: ID of the target Google Sheet \n",
    "        worksheet_name: Name of the worksheet to write to\n",
    "        \n",
    "    The function will resize the worksheet to match the DataFrame dimensions\n",
    "    and write all data starting from cell A1.\n",
    "    \"\"\"\n",
    "    google_sheets_df = df.copy()\n",
    "    google_sheets_df.fillna('',inplace=True)\n",
    "\n",
    "    gc = gspread.service_account()\n",
    "\n",
    "    sh = gc.open_by_key(google_sheets_id)\n",
    "\n",
    "    # Select the first worksheet\n",
    "    worksheet = sh.worksheet(worksheet_name)\n",
    "\n",
    "    # Convert DataFrame to list of lists (including column headers)\n",
    "    data = [google_sheets_df.columns.tolist()] + google_sheets_df.values.tolist()\n",
    "\n",
    "    # Minimize to just the data\n",
    "    num_rows = len(data)\n",
    "    num_cols = len(data[0])\n",
    "    worksheet.resize(rows=num_rows,cols=num_cols)\n",
    "\n",
    "    # Write data to the sheet, starting from A1\n",
    "    worksheet.update(data,value_input_option=\"USER_ENTERED\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# UTILITY FUNCTIONS FOR SCRAPING DIFFERENT BILL ASPECTS\n",
    "################################################################################\n",
    "\n",
    "def get_bill_history(scraper_config, bill_id, session_id):\n",
    "    \"\"\"\n",
    "    Retrieve the history information for a specific bill.\n",
    "    \n",
    "    Args:\n",
    "        scraper_config: Dictionary containing scraping configuration\n",
    "        bill_id: ID of the bill (e.g. 'HB1')\n",
    "        session_id: Legislative session ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing bill history information including status,\n",
    "        last action, and other metadata from the history page\n",
    "    \"\"\"\n",
    "    bill_url = f'{scraper_config['sources']['html']['history']}?LegSess={session_id}&Bill={bill_id}'\n",
    "    site_html = requests.get(bill_url,timeout=30).text\n",
    "    soup = BeautifulSoup(site_html, 'html.parser')\n",
    "\n",
    "    bill_history_dict = {}\n",
    "\n",
    "    validation_summary = soup.find('div', id=lambda x: x and 'validationSummary' in x)\n",
    "    if validation_summary is not None:\n",
    "        # This means the bill does not yet exist\n",
    "        bill_history_dict['status_value'] = 'Unassigned'\n",
    "        return bill_history_dict\n",
    "\n",
    "    bill_content = soup.find_all('div', id=lambda x: x and 'content' in x.lower())\n",
    "\n",
    "    if not bill_content or len(bill_content) < 2:\n",
    "        # This means the bill does not have the required information\n",
    "        bill_history_dict['status_value'] = 'Unassigned'\n",
    "        return bill_history_dict\n",
    "\n",
    "    # Ignoring the first div, because it just contains information we already know\n",
    "    bill_content = bill_content[1]\n",
    "    bill_info_tables = bill_content.find_all('table')\n",
    "\n",
    "    for table_html in bill_info_tables:\n",
    "        \n",
    "        for row in table_html.find_all('tr'):\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) == 2:\n",
    "                key = cells[0].get_text(strip=True).replace(':', '')\n",
    "                value = cells[1].get_text(strip=True)\n",
    "                bill_history_dict[key] = value\n",
    "    \n",
    "    # TO DO: Define Status Logic\n",
    "    bill_history_dict['status_value'] = 'Alive'\n",
    "    return bill_history_dict\n",
    "\n",
    "def get_bill_text(scraper_config, bill_id, session_id):\n",
    "    \"\"\"\n",
    "    Retrieve the text versions available for a specific bill.\n",
    "    \n",
    "    Args:\n",
    "        scraper_config: Dictionary containing scraping configuration\n",
    "        bill_id: ID of the bill (e.g. 'HB1')\n",
    "        session_id: Legislative session ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing URLs to introduced and most recent bill text PDFs,\n",
    "        as well as a DataFrame of all available text versions\n",
    "    \"\"\"\n",
    "    bill_text_url = f'{scraper_config['sources']['html']['text']}?LegSess={session_id}&Bill={bill_id}'\n",
    "    bill_text_dict = {'bill_id': bill_id, 'session_id':session_id} # gets filled in with information\n",
    "\n",
    "    site_html = requests.get(bill_text_url,timeout=30).text\n",
    "    soup = BeautifulSoup(site_html, 'html.parser')\n",
    "\n",
    "    validation_summary = soup.find('div', id=lambda x: x and 'validationSummary' in x)\n",
    "    if validation_summary is not None:\n",
    "        return {}\n",
    "\n",
    "    bill_text_info_container = soup.find('form', id='Form1') # Why is it just called \"form1\"??? That's not descriptive at all??\n",
    "    bill_info_tables = bill_text_info_container.find_all('table')\n",
    "\n",
    "    versions_table = bill_info_tables[0]\n",
    "    if 'not yet available' in versions_table.text.strip().lower():\n",
    "        return {}\n",
    "    \n",
    "    versions_df = bill_text_table_to_df(versions_table)\n",
    "\n",
    "    introduced_only_versions_df = versions_df[versions_df['Version'] == 'Introduced']\n",
    "    bill_text_dict = {\n",
    "        'introduced_text_pdf': 'https://capitol.texas.gov' + list(filter(lambda a: a['name'] == 'Adobe PDF', introduced_only_versions_df.iloc[0]['Bill']))[0]['URL'], # gross, sorry!\n",
    "        'recent_text_pdf': 'https://capitol.texas.gov' + list(filter(lambda a: a['name'] == 'Adobe PDF', versions_df[versions_df['Bill'] != ''].iloc[-1]['Bill']))[0]['URL'],\n",
    "        'text_versions_df' : versions_df\n",
    "    }\n",
    "    return bill_text_dict\n",
    "\n",
    "def get_bill_stages(scraper_config, bill_id, session_id):\n",
    "    \"\"\"\n",
    "    Retrieve the progression stages of a specific bill.\n",
    "    \n",
    "    Args:\n",
    "        scraper_config: Dictionary containing scraping configuration\n",
    "        bill_id: ID of the bill (e.g. 'HB1')\n",
    "        session_id: Legislative session ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing a list of stages the bill has gone through,\n",
    "        with each stage including number, status, date and continuation status\n",
    "    \"\"\"\n",
    "    bill_text_url = f'{scraper_config['sources']['html']['bill_stages']}?LegSess={session_id}&Bill={bill_id}'\n",
    "    bill_text_dict = {} # gets filled in with information\n",
    "\n",
    "    site_html = requests.get(bill_text_url,timeout=30).text\n",
    "    soup = BeautifulSoup(site_html, 'html.parser')\n",
    "\n",
    "    stages_div = soup.find('div', id='usrBillStages_pnlBillStages')\n",
    "    # Initialize empty list to store stages\n",
    "    bill_text_dict['stages'] = []\n",
    "\n",
    "    if stages_div:\n",
    "        # Find all stage boxes and continuations\n",
    "        stage_boxes = stages_div.find_all('div', class_='bill-status-box-complete')\n",
    "        stage_continuations = stages_div.find_all('div', class_='bill-status-continuation')\n",
    "\n",
    "        # Process each stage\n",
    "        for box, continuation in zip(stage_boxes, stage_continuations):\n",
    "            stage_num = box.find('div', class_='stage').text.strip().replace('Stage ', '')\n",
    "            status = box.find('div', class_='bill-status-status').text.strip()\n",
    "            date = box.find('div', class_='bill-status-date').text.strip()\n",
    "            \n",
    "            # Get image filename from continuation div\n",
    "            img = continuation.find('img')\n",
    "            img_src = img['src'].split('/')[-1] if img else None\n",
    "\n",
    "            stage = {\n",
    "                'stage_number': stage_num,\n",
    "                'status': status,\n",
    "                'date': date,\n",
    "                'continuation_image': img_src,\n",
    "                'after_status': img_src.replace('.gif','')\n",
    "            }\n",
    "            bill_text_dict['stages'].append(stage)\n",
    "    else:\n",
    "        # If no stages div found, return empty list\n",
    "        bill_text_dict['stages'] = []\n",
    "\n",
    "    return bill_text_dict\n",
    "\n",
    "def get_bill_actions(scraper_config, bill_id, session_id):\n",
    "    \"\"\"\n",
    "    Retrieve all actions taken on a specific bill.\n",
    "    \n",
    "    Args:\n",
    "        scraper_config: Dictionary containing scraping configuration\n",
    "        bill_id: ID of the bill (e.g. 'HB1')\n",
    "        session_id: Legislative session ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing a list of all actions taken on the bill,\n",
    "        with each action including chamber, description, date, time and other metadata\n",
    "    \"\"\"\n",
    "    bill_actions_url = f'{scraper_config['sources']['html']['actions']}?LegSess={session_id}&Bill={bill_id}'\n",
    "    bill_actions_dict = {} # gets filled in with information\n",
    "\n",
    "    site_html = requests.get(bill_actions_url,timeout=30).text\n",
    "    soup = BeautifulSoup(site_html, 'html.parser')\n",
    "    actions_form = soup.find('form', id='Form1')\n",
    "    actions_table = actions_form.find_all('table')[1]\n",
    "\n",
    "    # Initialize empty list to store actions\n",
    "    bill_actions_dict['actions'] = []\n",
    "\n",
    "    # Get all rows except header\n",
    "    action_rows = actions_table.find_all('tr')[1:]\n",
    "    \n",
    "    for row in action_rows:\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) >= 6:\n",
    "            # Get link if it exists\n",
    "            description_cell = cells[1]\n",
    "            description_link = description_cell.find('a')\n",
    "            link_url = description_link['href'] if description_link else None\n",
    "            \n",
    "            action = {\n",
    "                'chamber': cells[0].text.strip(),\n",
    "                'description': cells[1].text.strip(),\n",
    "                'description_link': link_url,\n",
    "                'comment': cells[2].text.strip(),\n",
    "                'date': cells[3].text.strip(),\n",
    "                'time': cells[4].text.strip(),\n",
    "                'journal_page': cells[5].text.strip()\n",
    "            }\n",
    "            bill_actions_dict['actions'].append(action)\n",
    "\n",
    "    return bill_actions_dict\n",
    "\n",
    "\n",
    "def get_bill_companions(scraper_config, bill_id, session_id):\n",
    "    \"\"\"\n",
    "    Retrieve companion bills for a specific bill.\n",
    "    \n",
    "    Args:\n",
    "        scraper_config: Dictionary containing scraping configuration\n",
    "        bill_id: ID of the bill (e.g. 'HB1')\n",
    "        session_id: Legislative session ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing a list of companion bills,\n",
    "        with each companion including bill number and link\n",
    "    \"\"\"\n",
    "    bill_companions_url = f'{scraper_config['sources']['html']['companions']}?LegSess={session_id}&Bill={bill_id}'\n",
    "    bill_companions_dict = {'companions': []} # gets filled in with information\n",
    "\n",
    "    site_html = requests.get(bill_companions_url,timeout=30).text\n",
    "    soup = BeautifulSoup(site_html, 'html.parser')\n",
    "    companions_form = soup.find('form', id='Form1')\n",
    "    print(bill_companions_url)\n",
    "\n",
    "    if len( companions_form.find_all('table')) < 2:\n",
    "        return {}\n",
    "    \n",
    "    companions_table = companions_form.find_all('table')[1]\n",
    "\n",
    "    # Find all companion bill rows (they have links in first td)\n",
    "    companion_rows = companions_table.find_all('tr')\n",
    "    \n",
    "    for i in range(0, len(companion_rows), 5):  # Each companion takes 5 rows\n",
    "        if i + 4 < len(companion_rows):\n",
    "            bill_link = companion_rows[i].find('a')\n",
    "            if bill_link:\n",
    "                companion = {\n",
    "                    'bill_number': bill_link.text.strip(),\n",
    "                    'bill_link': 'https://capitol.texas.gov/BillLookup/' + bill_link['href']\n",
    "                }\n",
    "                bill_companions_dict['companions'].append(companion)\n",
    "\n",
    "    return bill_companions_dict\n",
    "\n",
    "def get_bill_authors(scraper_config, bill_id, session_id):\n",
    "    \"\"\"\n",
    "    Retrieve authors and coauthors for a specific bill.\n",
    "    \n",
    "    Args:\n",
    "        scraper_config: Dictionary containing scraping configuration\n",
    "        bill_id: ID of the bill (e.g. 'HB1')\n",
    "        session_id: Legislative session ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing lists of primary authors and coauthors,\n",
    "        with each author including name and date added\n",
    "    \"\"\"\n",
    "    bill_authors_url = f'{scraper_config['sources']['html']['authors']}?LegSess={session_id}&Bill={bill_id}'\n",
    "    bill_authors_dict = {'primary_authors': [], 'coauthors': []}\n",
    "\n",
    "    # Get the HTML content\n",
    "    site_html = requests.get(bill_authors_url, timeout=30).text\n",
    "    soup = BeautifulSoup(site_html, 'html.parser')\n",
    "\n",
    "    # Get primary authors\n",
    "    primary_authors_table = soup.find('table', id='tblPrimaryAuthors')\n",
    "    if primary_authors_table:\n",
    "        for row in primary_authors_table.find_all('tr')[1:]:  # Skip header row\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 2:\n",
    "                author = {\n",
    "                    'name': cells[0].text.strip(),\n",
    "                    'date': cells[1].text.strip()\n",
    "                }\n",
    "                bill_authors_dict['primary_authors'].append(author)\n",
    "\n",
    "    # Get coauthors\n",
    "    coauthors_table = soup.find('table', id='tblCoauthors')\n",
    "    if coauthors_table:\n",
    "        for row in coauthors_table.find_all('tr')[1:]:  # Skip header row\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 2:\n",
    "                author = {\n",
    "                    'name': cells[0].text.strip(),\n",
    "                    'date': cells[1].text.strip()\n",
    "                }\n",
    "                bill_authors_dict['coauthors'].append(author)\n",
    "\n",
    "    return bill_authors_dict\n",
    "\n",
    "def get_bill_amendments(scraper_config, bill_id, session_id):\n",
    "    \"\"\"\n",
    "    Retrieve amendments for a specific bill.\n",
    "    \n",
    "    Args:\n",
    "        scraper_config: Dictionary containing scraping configuration\n",
    "        bill_id: ID of the bill (e.g. 'HB1')\n",
    "        session_id: Legislative session ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing lists of amendments with reading, number, author,\n",
    "        coauthor, type, action, date and text links\n",
    "    \"\"\"\n",
    "    bill_amendments_url = f'{scraper_config['sources']['html']['amendments']}?LegSess={session_id}&Bill={bill_id}'\n",
    "    bill_amendments_dict = {'amendments': []} # gets filled in with information\n",
    "\n",
    "    # Get the HTML content\n",
    "    site_html = requests.get(bill_amendments_url, timeout=30).text\n",
    "    soup = BeautifulSoup(site_html, 'html.parser')\n",
    "    \n",
    "    # Find the amendments table\n",
    "    table = soup.find('table', {'border': '1'})\n",
    "    if not table:\n",
    "        return bill_amendments_dict\n",
    "        \n",
    "    # Get all rows except header\n",
    "    amendment_rows = table.find_all('tr')[1:]\n",
    "    \n",
    "    for row in amendment_rows:\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) >= 8:\n",
    "            # Get text links if they exist\n",
    "            text_cell = cells[7]\n",
    "            html_link = text_cell.find('a', href=lambda x: x and 'html' in x.lower())\n",
    "            pdf_link = text_cell.find('a', href=lambda x: x and 'pdf' in x.lower())\n",
    "            \n",
    "            amendment = {\n",
    "                'reading': cells[0].text.strip(),\n",
    "                'number': cells[1].text.strip(),\n",
    "                'author': cells[2].text.strip(),\n",
    "                'coauthor': cells[3].text.strip(),\n",
    "                'type': cells[4].text.strip(),\n",
    "                'action': cells[5].text.strip(),\n",
    "                'date': cells[6].text.strip(),\n",
    "                'html_link': 'https://capitol.texas.gov/' + html_link['href'] if html_link else None,\n",
    "                'pdf_link': 'https://capitol.texas.gov/' + pdf_link['href'] if pdf_link else None\n",
    "            }\n",
    "            bill_amendments_dict['amendments'].append(amendment)\n",
    "\n",
    "    return bill_amendments_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amendments': [{'reading': 'S 2',\n",
       "   'number': 'F1',\n",
       "   'author': 'Hughes',\n",
       "   'coauthor': '',\n",
       "   'type': 'Amendment',\n",
       "   'action': 'Withdrawn',\n",
       "   'date': '3/13/2023',\n",
       "   'html_link': 'https://capitol.texas.gov//tlodocs/88R/amendments/html/SB00002S2F1.HTM',\n",
       "   'pdf_link': 'https://capitol.texas.gov//tlodocs/88R/amendments/pdf/SB00002S2F1.PDF'},\n",
       "  {'reading': 'S 2',\n",
       "   'number': 'F2',\n",
       "   'author': 'Gutierrez',\n",
       "   'coauthor': '',\n",
       "   'type': 'Amendment',\n",
       "   'action': 'Failed',\n",
       "   'date': '3/13/2023',\n",
       "   'html_link': 'https://capitol.texas.gov//tlodocs/88R/amendments/html/SB00002S2F2.HTM',\n",
       "   'pdf_link': 'https://capitol.texas.gov//tlodocs/88R/amendments/pdf/SB00002S2F2.PDF'}]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_result = get_bill_amendments(scraper_config, 'SB2', '88R')\n",
    "test_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://capitol.texas.gov/BillLookup/Companions.aspx?LegSess=89R&Bill=HB1\n",
      "SB1\n",
      "https://capitol.texas.gov/BillLookup/Companions.aspx?LegSess=89R&Bill=HB2\n",
      "SB2\n",
      "https://capitol.texas.gov/BillLookup/Companions.aspx?LegSess=89R&Bill=HB3\n",
      "SB3\n",
      "https://capitol.texas.gov/BillLookup/Companions.aspx?LegSess=89R&Bill=HB4\n",
      "SB4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bill Number</th>\n",
       "      <th>Caption</th>\n",
       "      <th>Bill History/Status</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Captions</th>\n",
       "      <th>Dead|Alive|Unassigned|Law</th>\n",
       "      <th>Latest Action Date</th>\n",
       "      <th>Latest Action Chamber</th>\n",
       "      <th>Latest Action</th>\n",
       "      <th>Link | All Texts</th>\n",
       "      <th>Recent Bill Text</th>\n",
       "      <th>Introduced Text</th>\n",
       "      <th>Companions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HB1</td>\n",
       "      <td>General Appropriations Bill.</td>\n",
       "      <td>https://capitol.texas.gov/BillLookup/History.a...</td>\n",
       "      <td>=HYPERLINK(\"https://capitol.texas.gov/BillLook...</td>\n",
       "      <td>https://capitol.texas.gov/BillLookup/Captions....</td>\n",
       "      <td>Alive</td>\n",
       "      <td>02/25/2025</td>\n",
       "      <td>H</td>\n",
       "      <td>=HYPERLINK(\"https://capitol.texas.gov/BillLook...</td>\n",
       "      <td>https://capitol.texas.gov/BillLookup/Text.aspx...</td>\n",
       "      <td>https://capitol.texas.gov/tlodocs/89R/billtext...</td>\n",
       "      <td>https://capitol.texas.gov/tlodocs/89R/billtext...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HB2</td>\n",
       "      <td>Relating to public education and public school...</td>\n",
       "      <td>https://capitol.texas.gov/BillLookup/History.a...</td>\n",
       "      <td>=HYPERLINK(\"https://capitol.texas.gov/BillLook...</td>\n",
       "      <td>https://capitol.texas.gov/BillLookup/Captions....</td>\n",
       "      <td>Alive</td>\n",
       "      <td>03/06/2025</td>\n",
       "      <td>H</td>\n",
       "      <td>=HYPERLINK(\"https://capitol.texas.gov/BillLook...</td>\n",
       "      <td>https://capitol.texas.gov/BillLookup/Text.aspx...</td>\n",
       "      <td>https://capitol.texas.gov/tlodocs/89R/billtext...</td>\n",
       "      <td>https://capitol.texas.gov/tlodocs/89R/billtext...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HB3</td>\n",
       "      <td>Relating to the establishment of an education ...</td>\n",
       "      <td>https://capitol.texas.gov/BillLookup/History.a...</td>\n",
       "      <td>=HYPERLINK(\"https://capitol.texas.gov/BillLook...</td>\n",
       "      <td>https://capitol.texas.gov/BillLookup/Captions....</td>\n",
       "      <td>Alive</td>\n",
       "      <td>03/11/2025</td>\n",
       "      <td>H</td>\n",
       "      <td>=HYPERLINK(\"https://capitol.texas.gov/BillLook...</td>\n",
       "      <td>https://capitol.texas.gov/BillLookup/Text.aspx...</td>\n",
       "      <td>https://capitol.texas.gov/tlodocs/89R/billtext...</td>\n",
       "      <td>https://capitol.texas.gov/tlodocs/89R/billtext...</td>\n",
       "      <td>=HYPERLINK(\"https://capitol.texas.gov/BillLook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HB4</td>\n",
       "      <td>Relating to the assessment of public school st...</td>\n",
       "      <td>https://capitol.texas.gov/BillLookup/History.a...</td>\n",
       "      <td>=HYPERLINK(\"https://capitol.texas.gov/BillLook...</td>\n",
       "      <td>https://capitol.texas.gov/BillLookup/Captions....</td>\n",
       "      <td>Alive</td>\n",
       "      <td>02/25/2025</td>\n",
       "      <td>H</td>\n",
       "      <td>=HYPERLINK(\"https://capitol.texas.gov/BillLook...</td>\n",
       "      <td>https://capitol.texas.gov/BillLookup/Text.aspx...</td>\n",
       "      <td>https://capitol.texas.gov/tlodocs/89R/billtext...</td>\n",
       "      <td>https://capitol.texas.gov/tlodocs/89R/billtext...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Bill Number                                            Caption  \\\n",
       "0         HB1                       General Appropriations Bill.   \n",
       "1         HB2  Relating to public education and public school...   \n",
       "2         HB3  Relating to the establishment of an education ...   \n",
       "3         HB4  Relating to the assessment of public school st...   \n",
       "\n",
       "                                 Bill History/Status  \\\n",
       "0  https://capitol.texas.gov/BillLookup/History.a...   \n",
       "1  https://capitol.texas.gov/BillLookup/History.a...   \n",
       "2  https://capitol.texas.gov/BillLookup/History.a...   \n",
       "3  https://capitol.texas.gov/BillLookup/History.a...   \n",
       "\n",
       "                                             Authors  \\\n",
       "0  =HYPERLINK(\"https://capitol.texas.gov/BillLook...   \n",
       "1  =HYPERLINK(\"https://capitol.texas.gov/BillLook...   \n",
       "2  =HYPERLINK(\"https://capitol.texas.gov/BillLook...   \n",
       "3  =HYPERLINK(\"https://capitol.texas.gov/BillLook...   \n",
       "\n",
       "                                            Captions  \\\n",
       "0  https://capitol.texas.gov/BillLookup/Captions....   \n",
       "1  https://capitol.texas.gov/BillLookup/Captions....   \n",
       "2  https://capitol.texas.gov/BillLookup/Captions....   \n",
       "3  https://capitol.texas.gov/BillLookup/Captions....   \n",
       "\n",
       "  Dead|Alive|Unassigned|Law Latest Action Date Latest Action Chamber  \\\n",
       "0                     Alive         02/25/2025                     H   \n",
       "1                     Alive         03/06/2025                     H   \n",
       "2                     Alive         03/11/2025                     H   \n",
       "3                     Alive         02/25/2025                     H   \n",
       "\n",
       "                                       Latest Action  \\\n",
       "0  =HYPERLINK(\"https://capitol.texas.gov/BillLook...   \n",
       "1  =HYPERLINK(\"https://capitol.texas.gov/BillLook...   \n",
       "2  =HYPERLINK(\"https://capitol.texas.gov/BillLook...   \n",
       "3  =HYPERLINK(\"https://capitol.texas.gov/BillLook...   \n",
       "\n",
       "                                    Link | All Texts  \\\n",
       "0  https://capitol.texas.gov/BillLookup/Text.aspx...   \n",
       "1  https://capitol.texas.gov/BillLookup/Text.aspx...   \n",
       "2  https://capitol.texas.gov/BillLookup/Text.aspx...   \n",
       "3  https://capitol.texas.gov/BillLookup/Text.aspx...   \n",
       "\n",
       "                                    Recent Bill Text  \\\n",
       "0  https://capitol.texas.gov/tlodocs/89R/billtext...   \n",
       "1  https://capitol.texas.gov/tlodocs/89R/billtext...   \n",
       "2  https://capitol.texas.gov/tlodocs/89R/billtext...   \n",
       "3  https://capitol.texas.gov/tlodocs/89R/billtext...   \n",
       "\n",
       "                                     Introduced Text  \\\n",
       "0  https://capitol.texas.gov/tlodocs/89R/billtext...   \n",
       "1  https://capitol.texas.gov/tlodocs/89R/billtext...   \n",
       "2  https://capitol.texas.gov/tlodocs/89R/billtext...   \n",
       "3  https://capitol.texas.gov/tlodocs/89R/billtext...   \n",
       "\n",
       "                                          Companions  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2  =HYPERLINK(\"https://capitol.texas.gov/BillLook...  \n",
       "3                                                NaN  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_PATH = './scraper_config.yaml'\n",
    "with open(CONFIG_PATH, 'r') as file:\n",
    "    scraper_config = yaml.safe_load(file)\n",
    "\n",
    "bill_id = 'SB11121'\n",
    "bill_id = 'SB1'\n",
    "session_id = scraper_config['info']['LegSess']\n",
    "\n",
    "def get_bill_dict(scraper_config, bill_id, session_id):\n",
    "    \"\"\"\n",
    "    Get all available information for a bill and return as dictionary.\n",
    "    \n",
    "    Args:\n",
    "        scraper_config: Dictionary containing scraping configuration\n",
    "        bill_id: ID of the bill (e.g. 'HB1')\n",
    "        session_id: Legislative session ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing all available information about the bill,\n",
    "        including history, text, stages, actions, companions, and authors\n",
    "    \"\"\"\n",
    "    bill_info_dict = {'bill_id': bill_id, 'session_id': session_id}\n",
    "    \n",
    "    # Get data from all available functions\n",
    "    bill_history_dict = get_bill_history(scraper_config, bill_id, session_id)\n",
    "    bill_text_dict = get_bill_text(scraper_config, bill_id, session_id)\n",
    "    bill_stages_dict = get_bill_stages(scraper_config, bill_id, session_id)\n",
    "    bill_actions_dict = get_bill_actions(scraper_config, bill_id, session_id)\n",
    "    bill_companions_dict = get_bill_companions(scraper_config, bill_id, session_id)\n",
    "    bill_authors_dict = get_bill_authors(scraper_config, bill_id, session_id)\n",
    "    bill_amendments_dict = get_bill_amendments(scraper_config, bill_id, session_id)\n",
    "\n",
    "    # Update main dictionary with all the data\n",
    "    bill_info_dict.update(bill_history_dict)\n",
    "    bill_info_dict.update(bill_text_dict)\n",
    "    bill_info_dict.update(bill_stages_dict)\n",
    "    bill_info_dict.update(bill_actions_dict)\n",
    "    bill_info_dict.update(bill_companions_dict)\n",
    "    bill_info_dict.update(bill_authors_dict)\n",
    "    bill_info_dict.update(bill_amendments_dict)\n",
    "    # Get URLS\n",
    "    info_urls = list(scraper_config['sources']['html'].keys())\n",
    "    for key in info_urls:\n",
    "        url = f\"{scraper_config['sources']['html'][key]}?LegSess={session_id}&Bill={bill_id}\"\n",
    "        bill_info_dict[f'{key}_url'] = url\n",
    "\n",
    "    return bill_info_dict\n",
    "    \n",
    "def clean_bill_dict(bill_info_dict):\n",
    "    # Last Action Parsing\n",
    "    if 'Last Action' in bill_info_dict.keys():\n",
    "        last_action_string = bill_info_dict['Last Action']\n",
    "        match = re.match(r\"(\\d{2}/\\d{2}/\\d{4})\\s+(\\S)\\s+(.*)\", last_action_string)\n",
    "        if match:\n",
    "            date, chamber, description = match.groups()\n",
    "            bill_info_dict['last_action_date'] = date\n",
    "            bill_info_dict['last_action_chamber'] = chamber\n",
    "            bill_info_dict['last_action_description'] = description\n",
    "\n",
    "    # Subjects\n",
    "    if 'Subjects' in bill_info_dict.keys():\n",
    "        subject_matches = re.findall(r\"([^()]+)\\s+\\((I\\d+)\\)\", bill_info_dict['Subjects'])\n",
    "        subjects_list = [{\"subject\": subject.strip(), \"code\": code} for subject, code in subject_matches]\n",
    "        bill_info_dict['subjects_list'] = subjects_list\n",
    "\n",
    "    if 'Author' in bill_info_dict.keys():\n",
    "        bill_info_dict['authors_hyperlink'] = f'=HYPERLINK(\"{bill_info_dict['authors_url']}\", \"{bill_info_dict['Author'].replace('\"','\"\"')}\")'\n",
    "    if 'last_action_description' in bill_info_dict.keys():\n",
    "        bill_info_dict['last_action_hyperlink'] = f'=HYPERLINK(\"{bill_info_dict['actions_url']}\", \"{bill_info_dict['last_action_description'].replace('\"','\"\"')}\")'\n",
    "    if 'status_value' == 'Unassigned':\n",
    "        bill_info_dict['text_url'] = None\n",
    "    if 'companions' in bill_info_dict.keys() and len(bill_info_dict['companions']) > 0:\n",
    "        bill_info_dict['companion_bill_hyperlink'] = f'=HYPERLINK(\"{bill_info_dict['companions'][0]['bill_link']}\", \"{bill_info_dict['companions'][0]['bill_number']}\")'\n",
    "    if 'amendments' in bill_info_dict.keys() and len(bill_info_dict['amendments']) > 0:\n",
    "        bill_info_dict['amendments_hyperlink'] = f'=HYPERLINK(\"{bill_info_dict['amendments_url']}\", \"{len(bill_info_dict['amendments'])}\")'\n",
    "\n",
    "    # Votes\n",
    "    if 'Vote' in bill_info_dict.keys():\n",
    "        vote_matches = re.findall(r'(\\w+(?: \\w+)*)=(\\d+)', bill_info_dict['Vote'])\n",
    "        bill_info_dict['vote_counts'] = {key: int(value) for key, value in vote_matches}\n",
    "\n",
    "    return bill_info_dict\n",
    "\n",
    "def clean_bills_df(raw_bills_df):\n",
    "    final_col_names = {\n",
    "        'bill_id':'Bill Number',\n",
    "        'Caption Text': 'Caption',\n",
    "        'history_url': 'Bill History/Status',\n",
    "        'authors_hyperlink': 'Authors',\n",
    "        'captions_url': 'Captions',\n",
    "        'status_value': 'Dead|Alive|Unassigned|Law',\n",
    "        'last_action_date': 'Latest Action Date',\n",
    "        'last_action_chamber':'Latest Action Chamber',\n",
    "        'last_action_hyperlink': 'Latest Action',\n",
    "        'text_url': 'Link | All Texts',\n",
    "        'recent_text_pdf': 'Recent Bill Text',\n",
    "        'introduced_text_pdf': 'Introduced Text',\n",
    "        'companion_bill_hyperlink': 'Companions',\n",
    "        'amendments_hyperlink': 'Amendments',\n",
    "    }\n",
    "    renamed_bills_df = raw_bills_df.rename(columns=final_col_names)\n",
    "    cols_order = ['Bill Number','Caption','Bill History/Status','Authors',\n",
    "                  'Captions','Dead|Alive|Unassigned|Law', 'Latest Action Date',\n",
    "                  'Latest Action Chamber','Latest Action', 'Link | All Texts',\n",
    "                  'Recent Bill Text', 'Introduced Text', 'Companions'\n",
    "                  ]\n",
    "    reduced_bills_df = renamed_bills_df[cols_order]\n",
    "    return reduced_bills_df.reindex(columns = cols_order)\n",
    "\n",
    "\n",
    "bills_list = []\n",
    "consecutive_misses_allowed = 8\n",
    "misses = 0\n",
    "\n",
    "curr_bill_num = 1\n",
    "#while misses < consecutive_misses_allowed:\n",
    "for i in range(1,5):\n",
    "    raw_bill = get_bill_dict(scraper_config, f'HB{curr_bill_num}', session_id)\n",
    "    curr_bill = clean_bill_dict(raw_bill)\n",
    "    print(f'SB{curr_bill_num}')\n",
    "\n",
    "    if curr_bill['status_value'] == 'Unassigned':\n",
    "        misses += 1\n",
    "        curr_bill_num += 1\n",
    "        continue\n",
    "    else:\n",
    "        misses = 0\n",
    "    \n",
    "    bills_list.append(curr_bill)\n",
    "    curr_bill_num += 1\n",
    "\n",
    "raw_bills_df = pd.DataFrame(bills_list)\n",
    "clean_bills = clean_bills_df(raw_bills_df)\n",
    "clean_bills.to_csv('bill_data.tsv', sep='\\t',index=False)\n",
    "clean_bills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "def write_df_to_gsheets(df, google_sheets_id, worksheet_name):\n",
    "\n",
    "    google_sheets_df = df.copy()\n",
    "    google_sheets_df.fillna('',inplace=True)\n",
    "\n",
    "    gc = gspread.service_account()\n",
    "\n",
    "    sh = gc.open_by_key(google_sheets_id)\n",
    "\n",
    "    # Select the first worksheet\n",
    "    worksheet = sh.worksheet(worksheet_name)\n",
    "\n",
    "    # Convert DataFrame to list of lists (including column headers)\n",
    "    data = [google_sheets_df.columns.tolist()] + google_sheets_df.values.tolist()\n",
    "\n",
    "    # Minimize to just the data\n",
    "    num_rows = len(data)\n",
    "    num_cols = len(data[0])\n",
    "    worksheet.resize(rows=num_rows,cols=num_cols)\n",
    "\n",
    "    # Write data to the sheet, starting from A1\n",
    "    worksheet.update(data,value_input_option=\"USER_ENTERED\")\n",
    "\n",
    "\n",
    "google_sheets_id = scraper_config['config']['google_sheets_id']\n",
    "worksheet_name = 'All House Bills'\n",
    "write_df_to_gsheets(clean_bills, google_sheets_id, worksheet_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "texas-leg-scraper-gCTcqI2u-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
